{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "class DQNetwork(nn.Module):\n",
      "    \"\"\"\n",
      "    Deep Q-Network that outputs both action values and communication signals.\n",
      "    \"\"\"\n",
      "    def __init__(self, input_size=10, hidden_size=64, output_size=5):\n",
      "        super(DQNetwork, self).__init__()\n",
      "        \n",
      "        # Shared feature extraction\n",
      "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
      "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
      "        \n",
      "        # Action output\n",
      "        self.action_head = nn.Linear(hidden_size, output_size)\n",
      "        \n",
      "        # Communication output\n",
      "        self.comm_head = nn.Linear(hidden_size, 1)\n",
      "    \n",
      "    def forward(self, x):\n",
      "        \"\"\"\n",
      "        Forward pass through the network.\n",
      "        \"\"\"\n",
      "        # Feature extraction\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        \n",
      "        # Action values\n",
      "        action_values = self.action_head(x)\n",
      "        \n",
      "        # Communication signal\n",
      "        comm_signal = torch.tanh(self.comm_head(x))\n",
      "        \n",
      "        return action_values, comm_signal\n",
      "\n",
      "class ReplayBuffer:\n",
      "    \"\"\"\n",
      "    Simple replay buffer for DQN.\n",
      "    \"\"\"\n",
      "    def __init__(self, capacity=10000):\n",
      "        self.capacity = capacity\n",
      "        self.buffer = []\n",
      "        self.position = 0\n",
      "    \n",
      "    def push(self, state, action, reward, next_state, done, comm_signal):\n",
      "        \"\"\"Add a transition to the buffer.\"\"\"\n",
      "        if len(self.buffer) < self.capacity:\n",
      "            self.buffer.append(None)\n",
      "        self.buffer[self.position] = (state, action, reward, next_state, done, comm_signal)\n",
      "        self.position = (self.position + 1) % self.capacity\n",
      "    \n",
      "    def sample(self, batch_size):\n",
      "        \"\"\"Sample a batch of transitions.\"\"\"\n",
      "        indices = np.random.choice(len(self.buffer), batch_size)\n",
      "        states, actions, rewards, next_states, dones, comms = zip(*[self.buffer[i] for i in indices])\n",
      "        \n",
      "        return (\n",
      "            torch.FloatTensor(np.array(states)),\n",
      "            torch.LongTensor(np.array(actions)).view(-1, 1),\n",
      "            torch.FloatTensor(np.array(rewards)).view(-1, 1),\n",
      "            torch.FloatTensor(np.array(next_states)),\n",
      "            torch.BoolTensor(np.array(dones)).view(-1, 1),\n",
      "            torch.FloatTensor(np.array(comms)).view(-1, 1)\n",
      "        )\n",
      "    \n",
      "    def __len__(self):\n",
      "        return len(self.buffer)\n"
     ]
    }
   ],
   "source": [
    "with open('/scratch1/srajasek/dl/modelss.py', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dD4W-qveYrwt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 0/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.00 | Epsilon: 1.00\n",
      "Episode 50/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.02 | Epsilon: 0.78\n",
      "Episode 100/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.06 | Epsilon: 0.61\n",
      "Episode 150/1000 | Reward: 10.00 | Success: True | Success Rate (100): 0.24 | Epsilon: 0.47\n",
      "Episode 200/1000 | Reward: 10.00 | Success: True | Success Rate (100): 0.42 | Epsilon: 0.37\n",
      "Episode 250/1000 | Reward: 10.00 | Success: True | Success Rate (100): 0.54 | Epsilon: 0.29\n",
      "Episode 300/1000 | Reward: 10.00 | Success: True | Success Rate (100): 0.50 | Epsilon: 0.22\n",
      "Episode 350/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.46 | Epsilon: 0.17\n",
      "Episode 400/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.57 | Epsilon: 0.13\n",
      "Episode 450/1000 | Reward: 10.00 | Success: True | Success Rate (100): 0.58 | Epsilon: 0.10\n",
      "Episode 500/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.56 | Epsilon: 0.10\n",
      "Episode 550/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.57 | Epsilon: 0.10\n",
      "Episode 600/1000 | Reward: 10.00 | Success: True | Success Rate (100): 0.51 | Epsilon: 0.10\n",
      "Episode 650/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.46 | Epsilon: 0.10\n",
      "Episode 700/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.44 | Epsilon: 0.10\n",
      "Episode 750/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.42 | Epsilon: 0.10\n",
      "Episode 800/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.39 | Epsilon: 0.10\n",
      "Episode 850/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.36 | Epsilon: 0.10\n",
      "Episode 900/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.44 | Epsilon: 0.10\n",
      "Episode 950/1000 | Reward: 0.00 | Success: False | Success Rate (100): 0.45 | Epsilon: 0.10\n",
      "Episode 999/1000 | Reward: 10.00 | Success: True | Success Rate (100): 0.44 | Epsilon: 0.10\n",
      "Model saved as dqn_net.pt\n",
      "Training results saved as training_results.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multi_agent_env import MultiAgentEnv\n",
    "from netmodelss import DQNetwork, ReplayBuffer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 0.001\n",
    "TARGET_UPDATE = 10\n",
    "NUM_EPISODES = 1000\n",
    "HIDDEN_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Initialize environment\n",
    "env = MultiAgentEnv()\n",
    "\n",
    "# Initialize networks\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Agent A networks\n",
    "policy_net_A = DQNetwork(input_size=10, hidden_size=HIDDEN_SIZE).to(device)\n",
    "target_net_A = DQNetwork(input_size=10, hidden_size=HIDDEN_SIZE).to(device)\n",
    "target_net_A.load_state_dict(policy_net_A.state_dict())\n",
    "target_net_A.eval()\n",
    "\n",
    "# Agent B networks\n",
    "policy_net_B = DQNetwork(input_size=10, hidden_size=HIDDEN_SIZE).to(device)\n",
    "target_net_B = DQNetwork(input_size=10, hidden_size=HIDDEN_SIZE).to(device)\n",
    "target_net_B.load_state_dict(policy_net_B.state_dict())\n",
    "target_net_B.eval()\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_A = optim.Adam(policy_net_A.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_B = optim.Adam(policy_net_B.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize replay buffers\n",
    "memory_A = ReplayBuffer(BUFFER_SIZE)\n",
    "memory_B = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "# Training metrics\n",
    "episode_rewards = []\n",
    "success_rates = []\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "def select_action(state, policy_net, epsilon):\n",
    "    \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        # Exploration: random action\n",
    "        action = np.random.randint(0, 5)\n",
    "        with torch.no_grad():\n",
    "            _, comm = policy_net(torch.FloatTensor(state).unsqueeze(0).to(device))\n",
    "            comm = comm.item()\n",
    "    else:\n",
    "        # Exploitation: best action\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values, comm = policy_net(state_tensor)\n",
    "            action = q_values.max(1)[1].item()\n",
    "            comm = comm.item()\n",
    "    return action, comm\n",
    "\n",
    "def optimize_model(policy_net, target_net, optimizer, memory):\n",
    "    \"\"\"Perform one step of optimization for a DQN.\"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "    \n",
    "    # Sample batch\n",
    "    states, actions, rewards, next_states, dones, _ = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Move to device\n",
    "    states = states.to(device)\n",
    "    actions = actions.to(device)\n",
    "    rewards = rewards.to(device)\n",
    "    next_states = next_states.to(device)\n",
    "    dones = dones.to(device)\n",
    "    \n",
    "    # Compute Q(s_t, a)\n",
    "    q_values, _ = policy_net(states)\n",
    "    q_values = q_values.gather(1, actions)\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    with torch.no_grad():\n",
    "        next_q_values, _ = target_net(next_states)\n",
    "        next_q_values = next_q_values.max(1, keepdim=True)[0]\n",
    "        expected_q_values = rewards + GAMMA * next_q_values * (~dones)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.smooth_l1_loss(q_values, expected_q_values)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping - FIXED VERSION\n",
    "    for param in policy_net.parameters():\n",
    "        if param.grad is not None:  # Check if gradient exists\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "successes = 0\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Reset environment\n",
    "    obs_A, obs_B = env.reset()\n",
    "    \n",
    "    # Reset episode metrics\n",
    "    episode_reward = 0\n",
    "    episode_success = False\n",
    "    comm_A, comm_B = 0.0, 0.0\n",
    "    \n",
    "    # Update epsilon\n",
    "    epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** episode))\n",
    "    \n",
    "    # Episode loop\n",
    "    while True:\n",
    "        # Select actions\n",
    "        action_A, new_comm_A = select_action(obs_A, policy_net_A, epsilon)\n",
    "        action_B, new_comm_B = select_action(obs_B, policy_net_B, epsilon)\n",
    "        \n",
    "        # Take action in environment\n",
    "        (next_obs_A, next_obs_B), reward, done = env.step(action_A, action_B, new_comm_A, new_comm_B)\n",
    "        \n",
    "        # Store transitions\n",
    "        memory_A.push(obs_A, action_A, reward, next_obs_A, done, new_comm_A)\n",
    "        memory_B.push(obs_B, action_B, reward, next_obs_B, done, new_comm_B)\n",
    "        \n",
    "        # Update observations and communication signals\n",
    "        obs_A, obs_B = next_obs_A, next_obs_B\n",
    "        comm_A, comm_B = new_comm_A, new_comm_B\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimize_model(policy_net_A, target_net_A, optimizer_A, memory_A)\n",
    "        optimize_model(policy_net_B, target_net_B, optimizer_B, memory_B)\n",
    "        \n",
    "        # Track reward and success\n",
    "        episode_reward += reward\n",
    "        if reward > 0:\n",
    "            episode_success = True\n",
    "            successes += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Update target networks\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net_A.load_state_dict(policy_net_A.state_dict())\n",
    "        target_net_B.load_state_dict(policy_net_B.state_dict())\n",
    "    \n",
    "    # Record metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    success_rates.append(1 if episode_success else 0)\n",
    "    \n",
    "    # Print progress\n",
    "    if episode % 50 == 0 or episode == NUM_EPISODES - 1:\n",
    "        success_rate = np.mean(success_rates[-100:]) if len(success_rates) >= 100 else np.mean(success_rates)\n",
    "        print(f\"Episode {episode}/{NUM_EPISODES} | \"\n",
    "              f\"Reward: {episode_reward:.2f} | \"\n",
    "              f\"Success: {episode_success} | \"\n",
    "              f\"Success Rate (100): {success_rate:.2f} | \"\n",
    "              f\"Epsilon: {epsilon:.2f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if len(success_rates) >= 100 and np.mean(success_rates[-100:]) >= 0.95:\n",
    "        print(f\"Early stopping at episode {episode} with success rate {np.mean(success_rates[-100:]):.2f}\")\n",
    "        break\n",
    "\n",
    "# Save the trained model (Agent A)\n",
    "scripted_model = torch.jit.script(policy_net_A)\n",
    "scripted_model.save(\"dqn_net.pt\")\n",
    "print(\"Model saved as dqn_net.pt\")\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Calculate moving average of success rate\n",
    "window_size = min(100, len(success_rates))\n",
    "moving_avg = [np.mean(success_rates[max(0, i-window_size+1):i+1]) for i in range(len(success_rates))]\n",
    "plt.plot(moving_avg)\n",
    "plt.title('Success Rate (Moving Average)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Training results saved as training_results.png\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12.5 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
